# CONNECT APACHE HIVE WITH PYTHON

Hive comes with a server interface called HiveServer2, 
and it also provides a Command Line Interface (CLI) called Beeline 
that can be used to connect to Hive operating on a local or remote server and execute HiveQL queries. 
The SQLLine CLI serves as the foundation for Beeline, a JDBC client.

Using IP address and port, Beeline enables us to establish a connection to Hive running on a nearby or distant server.

Steps: 
1. setup proxy settings:
navigate to the Hadoop core-site.xml configuration file and open this file with your favourite code editor, i use
[visual studio code]. How you installed Hadoop on your computer will determine where the file is located.

$ cd /Users/USERNAME/hadoop-3.3.5/etc/hadoop
$ code core-site.xml

2. edit this file with the below properties:

NOTE: the "hiveuser" in the code below is your username 
(your hive username. i.e the username you set when you configured hive in the hive-site.xml file)

 <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hiveuser.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hiveuser.groups</name>
        <value>*</value>
    </property>
</configuration>

3. Update Hive metadata configuration:
Update your Hive configuration file hive-site.xml:
set these below attributes of the file the below features:

<property>
    <name>hive.server2.logging.operation.enabled</name>
    <value>false</value>
    <description>When true, HS2 will save operation logs and make them available for clients</description>
  </property>
 <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    <description>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.</description>
  </property>
  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>localhost</value>
    <description>Bind host on which to run the HiveServer2 Thrift service.</description>
  </property>
<property>
    <name>hive.server2.thrift.http.port</name>
    <value>10001</value>
    <description>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'http'.</description>
  </property>

4. restart Hadoop and HiveServer2 services

$HIVE_HOME/bin/hive --service metastore &
$HIVE_HOME/bin/hive --service hiveserver2 &

5. connect beeline with hiverserver2:
beeline> !connect jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000: hiveuser
Enter password for jdbc:hive2://localhost:10000: hivepassword
Connected to: Apache Hive (version 3.1.3)
Driver: Hive JDBC (version 3.1.3)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> show databases;

6. if you encounter any issue or error, check the issue file of this respository and try to fix it.

7. access hiveserver2 via [hiveserver2](http://localhost:10002/)

8. connect hive with python to access hive data to work with in python:
$ !pip install pyhive
$ !pip install sasl
$ !pip install thrift
$ !pip install thrift_sasl

if you are using jupyter notebook to access python and you have gcc installed on your machine, you may get error when trying to install
sasl. use the below command instead:

$ conda install -c conda-forge sasl

from pyhive import hive
import sasl
import thrift
import thrift_sasl
import pandas as pd

9. access hive tables in python:

$ conn = hive.Connection(host="localhost", port=10000)
$ df = pd.read_sql("SELECT * FROM baci.book_ratings", conn)
$ df.head()

NOTE: "baci" in the df argument is my DB(database name) in hive and book_ratings is the table in the database.

boom! congratulations. you now have your big data pipeline, liking hadoop with python. you can receive data from far off into your python
environment and develop any system you want in python.

any error? check the error file and we can connect via linkedin

